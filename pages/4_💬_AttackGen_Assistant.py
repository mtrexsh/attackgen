import os
import pandas as pd
import streamlit as st
import re

from langchain.callbacks.manager import collect_runs
from langchain_community.llms import Ollama
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_mistralai.chat_models import ChatMistralAI
from langchain_openai import ChatOpenAI, AzureOpenAI
from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langsmith import Client, RunTree, traceable
from mitreattack.stix20 import MitreAttackData
from openai import OpenAI

# ------------------ Streamlit UI ------------------ #

st.set_page_config(page_title="AttackGen Assistant", page_icon=":speech_balloon:")

st.markdown("# <span style='color: #1DB954;'>AttackGen AssistantðŸ’¬</span>", unsafe_allow_html=True)

if 'last_scenario_text' in st.session_state and st.session_state['last_scenario']:
    input_scenario = st.session_state['last_scenario_text']
    with st.expander("Generated Scenario"):
        with st.container(height=400, border=True):
            st.markdown(input_scenario)

    # Create an empty container for the chat messages
    chat_container = st.empty()

    # Initialize st.session_state.messages if it doesn't exist
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Hi, I can help you update and ask questions about your incident response scenario."}
        ]

    # Display the chat messages in the empty container
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    def generate_response(user_input, chat_history):
        model_provider = st.session_state["chosen_model_provider"]

        if model_provider == "Azure OpenAI Service":
            azure_api_key = os.getenv('AZURE_OPENAI_API_KEY')
            azure_api_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
            azure_deployment_name = os.getenv('AZURE_DEPLOYMENT')
            azure_api_version = os.getenv('OPENAI_API_VERSION')
            llm = AzureOpenAI(api_key=azure_api_key,
                              azure_endpoint=azure_api_endpoint,
                              api_version=azure_api_version)
            messages = [
                SystemMessagePromptTemplate.from_template("""
                You are an AI assistant that helps users update and ask questions about their incident response scenario.
                Only respond to questions or requests relating to the scenario, or incident response testing in general.
                """).format(),
                HumanMessagePromptTemplate.from_template(
                    "Here is the scenario that the user previously generated:\n\n{input_scenario}\n\nChat history:\n{chat_history}\n\nUser: {user_input}"
                ).format(input_scenario=input_scenario, chat_history=chat_history, user_input=user_input)
            ]
            formatted_messages = []
            for message in messages:
                if hasattr(message, 'role') and hasattr(message, 'content'):
                    role = message.role
                    if role == 'human':
                        role = 'user'
                    formatted_messages.append({"role": role, "content": message.content})
                elif hasattr(message, 'type') and hasattr(message, 'content'):
                    role = message.type
                    if role == 'human':
                        role = 'user'
                    formatted_messages.append({"role": role, "content": message.content})
                else:
                    raise ValueError(f"Unsupported message format: {message}")
            response = llm.chat.completions.create(
                model=azure_deployment_name,
                messages=formatted_messages
            )
            return response.choices[0].message.content
        elif model_provider == "Google AI API":
            google_api_key = os.getenv('GOOGLE_API_KEY')
            model = os.getenv('GOOGLE_MODEL')
            llm = ChatGoogleGenerativeAI(google_api_key=google_api_key, model=model)
            messages = [
                SystemMessagePromptTemplate.from_template("""
                You are an AI assistant that helps users update and ask questions about their incident response scenario.
                Only respond to questions or requests relating to the scenario, or incident response testing in general.
                """).format(),
                HumanMessagePromptTemplate.from_template(
                    "Here is the scenario that the user previously generated:\n\n{input_scenario}\n\nChat history:\n{chat_history}\n\nUser: {user_input}"
                ).format(input_scenario=input_scenario, chat_history=chat_history, user_input=user_input)
            ]
            response = llm.invoke(messages)
            return response.content
        elif model_provider == "Mistral API":
            mistral_api_key = os.getenv('MISTRAL_API_KEY')
            model = os.getenv('MISTRAL_MODEL')
            llm = ChatMistralAI(mistral_api_key=mistral_api_key)
            messages = [
                SystemMessagePromptTemplate.from_template("""
                You are an AI assistant that helps users update and ask questions about their incident response scenario.
                Only respond to questions or requests relating to the scenario, or incident response testing in general.
                """).format(),
                HumanMessagePromptTemplate.from_template(
                    "Here is the scenario that the user previously generated:\n\n{input_scenario}\n\nChat history:\n{chat_history}\n\nUser: {user_input}"
                ).format(input_scenario=input_scenario, chat_history=chat_history, user_input=user_input)
            ]
            response = llm.invoke(messages, model=model)
            return response.content
        elif model_provider == "Groq API":
            groq_api_key = os.getenv('GROQ_API_KEY')
            model = os.getenv('GROQ_MODEL')
            llm = OpenAI(
                api_key=groq_api_key,
                base_url="https://api.groq.com/openai/v1",
            )
            messages = [
                SystemMessagePromptTemplate.from_template("""
                You are an AI assistant that helps users update and ask questions about their incident response scenario.
                Only respond to questions or requests relating to the scenario, or incident response testing in general.
                """).format(),
                HumanMessagePromptTemplate.from_template(
                    "Here is the scenario that the user previously generated:\n\n{input_scenario}\n\nChat history:\n{chat_history}\n\nUser: {user_input}"
                ).format(input_scenario=input_scenario, chat_history=chat_history, user_input=user_input)
            ]
            formatted_messages = []
            for message in messages:
                if hasattr(message, 'role') and hasattr(message, 'content'):
                    role = message.role
                    if role == 'human':
                        role = 'user'
                    formatted_messages.append({"role": role, "content": message.content})
                elif hasattr(message, 'type') and hasattr(message, 'content'):
                    role = message.type
                    if role == 'human':
                        role = 'user'
                    formatted_messages.append({"role": role, "content": message.content})
                else:
                    raise ValueError(f"Unsupported message format: {message}")
            
            response = llm.chat.completions.create(
                model=model,
                messages=formatted_messages
            )
            content = response.choices[0].message.content
            
            # Check if this is DeepSeek output with thinking tags
            if re.search(r'<think>(.*?)</think>', content, re.DOTALL):
                # Extract the thinking content and the rest of the response
                thinking_match = re.search(r'<think>(.*?)</think>', content, re.DOTALL)
                thinking_content = thinking_match.group(1).strip()
                response_text = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
                
                # Display thinking content in an expander
                with st.expander("View Model's Reasoning"):
                    st.markdown(thinking_content)
                
                # Clean up the response text by removing code block markers if present
                response_text = re.sub(r'^```\w*\n|```$', '', response_text, flags=re.MULTILINE).strip()
                return response_text
            else:
                # If no thinking tags, clean up and return the entire content
                return re.sub(r'^```\w*\n|```$', '', content, flags=re.MULTILINE).strip()
        elif model_provider == "Ollama":
            model = os.getenv('OLLAMA_MODEL')
            llm = Ollama(model=model)
            prompt = (f"""
            You are an AI assistant that helps users update and ask questions about their incident response testing scenario.
            Only respond to questions or requests relating to the scenario, or incident response testing in general.\n\n
            Here is the scenario that the user previously generated:\n\n{input_scenario}\n\nChat history:\n{chat_history}\n\nUser: {user_input}"
            """)
            llm_result = llm.generate(prompts=[prompt])
            response = llm_result.generations[0][0].text
            return response
        elif model_provider == "Custom":
            # Fetch custom provider details from session state
            custom_api_key = st.session_state.get('custom_api_key')
            custom_model_name = st.session_state.get('custom_model_name')
            custom_base_url = st.session_state.get('custom_base_url')

            if not custom_base_url:
                return "Error: Custom Base URL not set in sidebar."
            if not custom_model_name:
                return "Error: Custom Model Name not set in sidebar."

            # Conditionally prepare client arguments
            client_args = {
                "base_url": custom_base_url,
            }
            if custom_api_key:
                client_args["api_key"] = custom_api_key

            try:
                llm = OpenAI(**client_args)

                # Prepare messages in OpenAI format
                messages_for_api = [
                    {
                        "role": "system",
                        "content": "You are an AI assistant that helps users update and ask questions about their incident response scenario. Only respond to questions or requests relating to the scenario, or incident response testing in general."
                    },
                    {
                        "role": "user",
                        "content": f"Here is the scenario that the user previously generated:\n\n{input_scenario}\n\nChat history:\n{chat_history}\n\nUser: {user_input}"
                    }
                ]

                response = llm.chat.completions.create(
                    model=custom_model_name,
                    messages=messages_for_api,
                    temperature=0.7, # Match previous curl example
                    # max_tokens=-1, # Consider if needed, OpenAI default is usually fine
                    stream=False
                )
                return response.choices[0].message.content
            except Exception as e:
                # import traceback
                st.error(f"Error communicating with Custom API: {e}")
                # st.text(traceback.format_exc())
                return "Error: Failed to get response from Custom API."

        else: # Default to OpenAI API
            openai_api_key = st.session_state.get('openai_api_key')
            model_name = st.session_state.get('model_name')
            
            if model_name in ["o3-mini","o1", "o1-mini"]:
                llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=model_name, temperature=1)
                system_content = """
                You are an AI assistant that helps users update and ask questions about their incident response scenario.
                Only respond to questions or requests relating to the scenario, or incident response testing in general.
                """
                messages = [
                    HumanMessagePromptTemplate.from_template(
                        f"{system_content}\n\nHere is the scenario that the user previously generated:\n\n{{input_scenario}}\n\nChat history:\n{{chat_history}}\n\nUser: {{user_input}}"
                    ).format(input_scenario=input_scenario, chat_history=chat_history, user_input=user_input)
                ]
            else:
                llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=model_name)
                messages = [
                    SystemMessagePromptTemplate.from_template(
                        "You are an AI assistant that helps users update and ask questions about their incident response scenario."
                    ).format(),
                    HumanMessagePromptTemplate.from_template(
                        "Here is the scenario that the user previously generated:\n\n{input_scenario}\n\nChat history:\n{chat_history}\n\nUser: {user_input}"
                    ).format(input_scenario=input_scenario, chat_history=chat_history, user_input=user_input)
                ]
            
            response = llm.generate(messages=[messages]).generations[0][0].text
            return response

    if prompt := st.chat_input("Type your message here..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            response = generate_response(prompt, "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[:-1]]))
            st.markdown(response)

        st.session_state.messages.append({"role": "assistant", "content": response})
    
    # Function to handle clearing the conversation
    def clear_conversation():
        st.session_state.messages = [
            {"role": "assistant", "content": "Hi, I can help you update and ask questions about your incident response scenario."}
        ]
        # Clear the chat bubbles from the UI
        chat_container.empty()

        # Re-render the initial assistant message
        with chat_container:
            with st.chat_message("assistant"):
                st.markdown(st.session_state.messages[0]["content"])

    # Add the "Clear Conversation" button at the bottom of the screen
    with st.container():
        if st.button("Clear Conversation", key='clear_button'):
            clear_conversation()

else:
    st.info("No scenario found. Please generate a scenario first.")